{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Wrangle Report </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrangle and analyze data project is one of the data analyst nanodegree program projects. The project aims to apply data wrangling skills to the tweet archive of WeRateDogs account to create exciting and trustworthy analyses and visualizations. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. The data wrangling contains four primary steps:\n",
    "\n",
    "<b><ol >\n",
    "    <li> Gathering data </li>\n",
    "    <li> Assessing data </li>\n",
    "    <li> Cleaning data </li>\n",
    "    <li> Storing data </li>\n",
    "</ol></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gathering data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in data wrangling is gathering data. This step varies between different projects. We came across three different sources of data with Jupyter Notebook:\n",
    "\n",
    "<ul>\n",
    "    <li> <b>twitter_archive_enhanced.csv file:</b> the file was downloaded manually from a given link. Then, it was imported using a CSV pandas method. </li> \n",
    "    <li> <b>image_predictions.tsv file:</b> the file was downloaded programmatically using the request method. Then, it was imported using a CSV pandas method with specifying separated method. </li>  \n",
    "    <li> <b>tweet_json.txt file:</b> this file resulted from using the tweet IDs of the first file mentioned in the gathering to querying the Twitter API using the Tweepy library. Then, the file was converted into pandas DataFrame by reading the file line by line.  </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Assessing data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessing data mean searching for quality and tidiness issues on the data. The assessing was performed visually and programmatically both done in Jupyter.\n",
    "\n",
    "Several issues were detected and listed below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality issues\n",
    "\n",
    "##### `df_archive` table\n",
    "\n",
    "1. Wrong datatype for tweet_id and timestamp columns\n",
    "\n",
    "2. None values in name column instead of Nan\n",
    "\n",
    "3. Some missing values in expanded_url column\n",
    "\n",
    "4. Missing data in the four last columns (doggo, floofer, pupper, and puppo) represented as None string\n",
    "\n",
    "5. Incorrect names with lowercase in name column like a, an, and such\n",
    "\n",
    "6. Rating_denominator not always have the value 10\n",
    "\n",
    "7. Only want original ratings (no retweets or replies)\n",
    "\n",
    "##### `df_img_prediction` table\n",
    "\n",
    "1. Unclear column names(p1,p1_conf,p1_dog,...,and p3_dog)\n",
    "\n",
    "2. No consistant format for prediction names in p1,p2, and p3 columns (some names start with a lowercase)\n",
    "\n",
    "3. Wrong datatype for tweet_id column\n",
    " \n",
    "##### `df_tweet_api` table\n",
    "\n",
    "1. Wrong datatype for tweet_id and timestamp columns\n",
    "\n",
    "\n",
    "### Tidiness issues\n",
    "\n",
    "1. One variable in four columns in `df_archive` table (dog_stage column)\n",
    "\n",
    "2. The three tables are related (forms a table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cleaning data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was cleaned using the define-code-test framework for each data issue. The quality and tidiness problems on the data mentioned above was cleaned by using multiple pandas methods, including merging to create a master dataframe, replacing all incorrect data, and renaming the column name to be more representative names, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Storing data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final cleaned dataframe was stored in CSV format with to_csv method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project was a good practice to put what I have learned from the course. Now, I feel confident to build a well-structured wrangling processing notebook with details in every part: detailed steps in how I gathered, assessed, and cleaned the data for every problem. \n",
    "\n",
    "The analyzing and visualizing part will be in another file called act_report.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'wrangle_report.ipynb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
